{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "from typing import List, Optional\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import binned_statistic\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "\n",
    "sns.set()"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "KlGPRsLH6J0jPn9a9IG77B",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def load_data(mode: str,\n",
    "              test_share: float,\n",
    "              val_share: float,\n",
    "              seed: int = 42,\n",
    "              drop_gammas=False,\n",
    "              binary_classes=False,\n",
    "              apply_cuts=False,\n",
    "              digitize=False) -> (np.ndarray, np.ndarray, np.ndarray,\n",
    "                                  np.ndarray, np.ndarray, np.ndarray,\n",
    "                                  np.ndarray, np.ndarray, np.ndarray,\n",
    "                                  Optional[List[float]]):\n",
    "    files_to_download = [f'{mode}_matrices.npz',\n",
    "                         f'{mode}_features.npz',\n",
    "                         f'{mode}_true_features.npz']\n",
    "    for filename in files_to_download:\n",
    "        os.makedirs('data', exist_ok=True)\n",
    "        if not os.path.exists(f'data/{filename}'):\n",
    "            print(f'Downloading {filename}... ', end='')\n",
    "            urlretrieve(\n",
    "                f'https://kascade-sim-data.s3.eu-central-1.amazonaws.com/{filename}',\n",
    "                f'data/{filename}')\n",
    "            print('Done!')\n",
    "\n",
    "    matrices = np.load(f'data/{files_to_download[0]}')['matrices']\n",
    "    features = np.load(f'data/{files_to_download[1]}')['features']\n",
    "    true_features = np.load(f'data/{files_to_download[2]}')['true_features']\n",
    "\n",
    "    matrices = matrices[..., 1:]\n",
    "\n",
    "    if binary_classes:\n",
    "        # prepare particle id for binary classification\n",
    "        true_features[:, 1][true_features[:, 1] == 1] = 0\n",
    "        true_features[:, 1][true_features[:, 1] != 0] = 1\n",
    "    else:\n",
    "        true_features[:, 1][true_features[:, 1] == 1] = 0\n",
    "        true_features[:, 1][true_features[:, 1] == 14] = 1\n",
    "        true_features[:, 1][true_features[:, 1] == 402] = 2\n",
    "        true_features[:, 1][true_features[:, 1] == 1206] = 3\n",
    "        true_features[:, 1][true_features[:, 1] == 2814] = 4\n",
    "        true_features[:, 1][true_features[:, 1] == 5626] = 5\n",
    "\n",
    "    # right now we're gonna detect only particle type\n",
    "    part_class = true_features[:, 1]\n",
    "\n",
    "    if drop_gammas:\n",
    "        drop_mask = part_class != 0\n",
    "        matrices = matrices[drop_mask]\n",
    "        features = features[drop_mask]\n",
    "        true_features = true_features[drop_mask]\n",
    "        part_class = part_class[drop_mask]\n",
    "        part_class -= 1\n",
    "    '''\n",
    "    features: ['part_type', 'E', 'Xc', 'Yc', 'core_dist', 'Ze', 'Az', 'Ne', 'Nmu', 'Age']\n",
    "    true_features: ['E', 'part_type', 'Xc', 'Yc', 'Ze', 'Az', 'Ne', 'Np', 'Nmu', 'Nh']\n",
    "    '''\n",
    "    if apply_cuts:\n",
    "        drop_mask = ((features[:, 5] < 18)\n",
    "                     * (features[:, 7] > 4.8)\n",
    "                     * (features[:, 8] > 3.6)\n",
    "                     * (features[:, 9] < 1.48)\n",
    "                     * (features[:, 9] > 0.2))\n",
    "        matrices = matrices[drop_mask]\n",
    "        features = features[drop_mask]\n",
    "        true_features = true_features[drop_mask]\n",
    "        part_class = part_class[drop_mask]\n",
    "\n",
    "    matrices = matrices.reshape(matrices.shape[0], -1)\n",
    "    vals = np.unique(true_features[:, [0]], axis=0)\n",
    "    random_mask = np.random.default_rng(seed).random(vals.shape)\n",
    "    is_train = np.in1d(true_features[:, [0]],\n",
    "                       vals[random_mask > (test_share + val_share)])\n",
    "    is_test = np.in1d(true_features[:, [0]],\n",
    "                      vals[random_mask < test_share])\n",
    "    is_val = np.invert(is_train + is_test)\n",
    "    del random_mask, vals\n",
    "    matrices_train = matrices[is_train]\n",
    "    matrices_test = matrices[is_test]\n",
    "    matrices_val = matrices[is_val]\n",
    "    del matrices\n",
    "    if digitize:\n",
    "        digitize_depth = 1000000\n",
    "        arr = np.array(random.choices(matrices_train, k=100000))\n",
    "        splits = np.array_split(np.sort(arr.ravel()), digitize_depth * 2)\n",
    "        cutoffs = [x[-1] for x in splits][:-1]\n",
    "        discrete = np.digitize(matrices_train, cutoffs, right=True)\n",
    "        matrices_train = discrete / digitize_depth - 1\n",
    "        discrete = np.digitize(matrices_test, cutoffs, right=True)\n",
    "        matrices_test = discrete / digitize_depth - 1\n",
    "        discrete = np.digitize(matrices_val, cutoffs, right=True)\n",
    "        matrices_val = discrete / digitize_depth - 1\n",
    "        del arr, splits, discrete, digitize_depth\n",
    "    else:\n",
    "        cutoffs = None\n",
    "    class_train = part_class[is_train]\n",
    "    class_test = part_class[is_test]\n",
    "    class_val = part_class[is_val]\n",
    "    features_train = features[is_train]\n",
    "    features_test = features[is_test]\n",
    "    features_val = features[is_val]\n",
    "    true_features_train = true_features[is_train]\n",
    "    true_features_test = true_features[is_test]\n",
    "    true_features_val = true_features[is_val]\n",
    "    del true_features, part_class\n",
    "    gc.collect()\n",
    "    return (matrices_train, matrices_test, matrices_val,\n",
    "            class_train, class_test, class_val,\n",
    "            features_train, features_test, features_val, cutoffs,\n",
    "            true_features_train, true_features_test, true_features_val)"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "KnwqgsjTlUDQpVlJnH7F9L",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evalute_predictions(test_preds, true_features, title=''):\n",
    "    test_true = true_features[:, 1]\n",
    "    test_energy = true_features[:, 0]\n",
    "    test_theta = true_features[:, 4]\n",
    "\n",
    "    # Plot figures\n",
    "    fig, axs = plt.subplots(2, 3, tight_layout=True, figsize=(21, 12))\n",
    "    axs = axs.flatten()\n",
    "    nbins = 8\n",
    "    bins_range = (14, 18)\n",
    "    E = test_energy\n",
    "    N, energy_bins = np.histogram(E, bins=nbins, range=bins_range)\n",
    "    energy_bins_centers = energy_bins[1:] - 0.5 * (energy_bins[1] - energy_bins[0])\n",
    "    energy_bins_half_width = 0.5 * (energy_bins[1] - energy_bins[0])\n",
    "\n",
    "    for i, (angle_min, angle_max) in zip(range(3), ((0, 20), (20, 40), (40, 60))):\n",
    "        cond = np.where((test_theta >= angle_min) & (test_theta < angle_max) & (test_true == 0) & (test_preds == 0))\n",
    "        correctly_predicted_gamma = \\\n",
    "            binned_statistic(E[cond], test_preds[cond], statistic='count', bins=nbins, range=bins_range)[0]\n",
    "\n",
    "        cond = np.where((test_theta >= angle_min) & (test_theta < angle_max) & (test_preds == 0))\n",
    "        all_predicted_gamma = \\\n",
    "            binned_statistic(E[cond], test_preds[cond], statistic='count', bins=nbins, range=bins_range)[0]\n",
    "\n",
    "        cond = np.where((test_theta >= angle_min) & (test_theta < angle_max) & (test_true == 0))\n",
    "        all_true_gamma = binned_statistic(E[cond], test_preds[cond], statistic='count', bins=nbins, range=bins_range)[0]\n",
    "\n",
    "        survival_fraction_gamma = correctly_predicted_gamma / (all_true_gamma + 0.001)\n",
    "        survival_fraction_gamma_error = np.sqrt(correctly_predicted_gamma) / (all_true_gamma + 0.001)\n",
    "\n",
    "        axs[i].errorbar(energy_bins_centers,\n",
    "                        survival_fraction_gamma,\n",
    "                        xerr=energy_bins_half_width,\n",
    "                        yerr=survival_fraction_gamma_error,\n",
    "                        fmt='o', capsize=3, capthick=3, ms=10, label='$ \\\\gamma $')\n",
    "\n",
    "        protons_predicted_as_gamma = all_predicted_gamma - correctly_predicted_gamma\n",
    "        protons_predicted_as_gamma_uplim = protons_predicted_as_gamma == 0\n",
    "        protons_predicted_as_gamma[protons_predicted_as_gamma_uplim] = 1\n",
    "\n",
    "        cond = np.where((test_theta >= angle_min) & (test_theta < angle_max) & (test_true == 1))\n",
    "        all_true_protons = binned_statistic(E[cond], test_preds[cond], statistic='count', bins=nbins, range=bins_range)[\n",
    "            0]\n",
    "\n",
    "        survival_fraction_protons = protons_predicted_as_gamma / (all_true_protons + 0.001)\n",
    "        survival_fraction_protons_error = np.sqrt(protons_predicted_as_gamma) / (all_true_protons + 0.001)\n",
    "\n",
    "        survival_fraction_protons_error[protons_predicted_as_gamma_uplim] = 0.5 / (all_true_protons[\n",
    "                                                                                       protons_predicted_as_gamma_uplim] + 0.0001)\n",
    "\n",
    "        axs[i].errorbar(energy_bins_centers,\n",
    "                        survival_fraction_protons,\n",
    "                        xerr=energy_bins_half_width,\n",
    "                        yerr=survival_fraction_protons_error,\n",
    "                        uplims=protons_predicted_as_gamma_uplim,\n",
    "                        fmt='o', capsize=3, capthick=3, ms=10, label='$p$')\n",
    "\n",
    "        axs[i].semilogy()\n",
    "        axs[i].legend(fontsize=17)\n",
    "        axs[i].xaxis.set_tick_params(labelsize=17)\n",
    "        axs[i].yaxis.set_tick_params(labelsize=17)\n",
    "        axs[i].set_xlabel('', fontsize=20)\n",
    "        axs[i].set_ylabel('survival fraction', fontsize=20)\n",
    "        axs[i].set_title(f'$ \\\\theta $ range: {angle_min} - {angle_max} deg', fontsize=15)\n",
    "        fig.suptitle('upper: survival fraction of $ \\\\gamma $ and p vs energy\\nlower: energy spectra of the events',\n",
    "                     fontsize=25)\n",
    "\n",
    "    for i, (angle_min, angle_max) in zip(range(3, 6), ((0, 20), (20, 40), (40, 60))):\n",
    "        cond = np.where((test_theta >= angle_min) & (test_theta < angle_max) & (test_true == 0))\n",
    "        sns.histplot(x=E[cond],\n",
    "                     bins=nbins,\n",
    "                     binrange=bins_range,\n",
    "                     log_scale=(False, False),\n",
    "                     element='step',\n",
    "                     fill=False,\n",
    "                     lw=3,\n",
    "                     ax=axs[i],\n",
    "                     label='$ \\\\gamma $')\n",
    "\n",
    "        cond = np.where((test_theta >= angle_min) & (test_theta < angle_max) & (test_true == 1))\n",
    "        sns.histplot(x=E[cond],\n",
    "                     bins=nbins,\n",
    "                     binrange=bins_range,\n",
    "                     log_scale=(False, False),\n",
    "                     element='step',\n",
    "                     fill=False,\n",
    "                     lw=3,\n",
    "                     ax=axs[i],\n",
    "                     label='$ p $')\n",
    "\n",
    "        axs[i].semilogy()\n",
    "        axs[i].legend(fontsize=17)\n",
    "        axs[i].xaxis.set_tick_params(labelsize=17)\n",
    "        axs[i].yaxis.set_tick_params(labelsize=17)\n",
    "        axs[i].set_xlabel('lg($E_0$/eV)', fontsize=20)\n",
    "        axs[i].set_ylabel('number of events', fontsize=20)"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "j6FCialIDttelf0PomH7Si",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "(matrices_train, matrices_test, matrices_val,\n",
    " class_train, class_test, class_val,\n",
    " features_train, features_test, features_val, cutoffs,\n",
    " true_features_train, true_features_test, true_features_val) = load_data(mode='qgs_gm_pr_v2',\n",
    "                                                                         test_share=0.2,\n",
    "                                                                         val_share=0.2)\n",
    "matrices_train = np.concatenate([matrices_train, features_train[:, [5, 6]]], axis=1)\n",
    "matrices_test = np.concatenate([matrices_test, features_test[:, [5, 6]]], axis=1)\n",
    "matrices_val = np.concatenate([matrices_val, features_val[:, [5, 6]]], axis=1)\n",
    "\n",
    "mean = matrices_train.mean(axis=0)\n",
    "std = matrices_train.std(axis=0)\n",
    "std[std == 0] = 1  # avoid division by zero\n",
    "matrices_train = (matrices_train - mean) / std\n",
    "matrices_test = (matrices_test - mean) / std\n",
    "matrices_val = (matrices_val - mean) / std"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "SDmYJQGyBzwwXw1A8IeKMe",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "activation = 'selu'\n",
    "x = model_input = tf.keras.Input(shape=(514,))\n",
    "x = layers.Dense(512)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation(activation)(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "x = layers.Dense(512)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation(activation)(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "out_layer = layers.Dense(2, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs=[model_input], outputs=[out_layer])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "history = model.fit(matrices_train,\n",
    "                    class_train,\n",
    "                    validation_data=(matrices_val, class_val),\n",
    "                    epochs=500,\n",
    "                    shuffle=True,\n",
    "                    verbose=1,\n",
    "                    batch_size=1024,\n",
    "                    callbacks=[\n",
    "                        tf.keras.callbacks.TensorBoard('logs/baseline_cnn'),\n",
    "                        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                            monitor='val_accuracy', factor=0.5, patience=15, verbose=1,\n",
    "                            min_delta=0.0001, cooldown=0\n",
    "                        ),\n",
    "                        tf.keras.callbacks.EarlyStopping(\n",
    "                            monitor='val_accuracy', min_delta=0, patience=65, verbose=1),\n",
    "                        tf.keras.callbacks.ModelCheckpoint(\n",
    "                            'best_weights.h5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "                    ],\n",
    "                    )\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "eHPAzSVj5pf3oJtbLMwyOu",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.load_weights('best_weights.h5')\n",
    "model.evaluate(matrices_test, class_test)\n",
    "preds = model.predict(matrices_test, batch_size=1024).argmax(1)\n",
    "evalute_predictions(preds, true_features_test)"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "wuYMyGAVylcYNQOtSLhFZs",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python"
  },
  "datalore": {
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "base_environment": "default",
   "packages": [],
   "report_row_ids": [],
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
