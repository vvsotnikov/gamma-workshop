{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "from typing import Optional, List\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import binned_statistic\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "sns.set()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "KlGPRsLH6J0jPn9a9IG77B",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def load_data(mode: str,\n",
    "              test_share: float,\n",
    "              val_share: float,\n",
    "              seed: int = 42,\n",
    "              drop_gammas=False,\n",
    "              binary_classes=False,\n",
    "              apply_cuts=False,\n",
    "              digitize=False) -> (np.ndarray, np.ndarray, np.ndarray,\n",
    "                                  np.ndarray, np.ndarray, np.ndarray,\n",
    "                                  np.ndarray, np.ndarray, np.ndarray,\n",
    "                                  Optional[List[float]]):\n",
    "    files_to_download = [f'{mode}_matrices.npz',\n",
    "                         f'{mode}_features.npz',\n",
    "                         f'{mode}_true_features.npz']\n",
    "    for filename in files_to_download:\n",
    "        os.makedirs('data', exist_ok=True)\n",
    "        if not os.path.exists(f'data/{filename}'):\n",
    "            print(f'Downloading {filename}... ', end='')\n",
    "            urlretrieve(\n",
    "                f'https://kascade-sim-data.s3.eu-central-1.amazonaws.com/{filename}',\n",
    "                f'data/{filename}')\n",
    "            print('Done!')\n",
    "\n",
    "    matrices = np.load(f'data/{files_to_download[0]}')['matrices']\n",
    "    features = np.load(f'data/{files_to_download[1]}')['features']\n",
    "    true_features = np.load(f'data/{files_to_download[2]}')['true_features']\n",
    "\n",
    "    matrices = matrices[..., 1:]\n",
    "\n",
    "    if binary_classes:\n",
    "        # prepare particle id for binary classification\n",
    "        true_features[:, 1][true_features[:, 1] == 1] = 0\n",
    "        true_features[:, 1][true_features[:, 1] != 0] = 1\n",
    "    else:\n",
    "        true_features[:, 1][true_features[:, 1] == 1] = 0\n",
    "        true_features[:, 1][true_features[:, 1] == 14] = 1\n",
    "        true_features[:, 1][true_features[:, 1] == 402] = 2\n",
    "        true_features[:, 1][true_features[:, 1] == 1206] = 3\n",
    "        true_features[:, 1][true_features[:, 1] == 2814] = 4\n",
    "        true_features[:, 1][true_features[:, 1] == 5626] = 5\n",
    "\n",
    "    # right now we're gonna detect only particle type\n",
    "    part_class = true_features[:, 1]\n",
    "\n",
    "    if drop_gammas:\n",
    "        drop_mask = part_class != 0\n",
    "        matrices = matrices[drop_mask]\n",
    "        features = features[drop_mask]\n",
    "        true_features = true_features[drop_mask]\n",
    "        part_class = part_class[drop_mask]\n",
    "        part_class -= 1\n",
    "    '''\n",
    "    features: ['part_type', 'E', 'Xc', 'Yc', 'core_dist', 'Ze', 'Az', 'Ne', 'Nmu', 'Age']\n",
    "    true_features: ['E', 'part_type', 'Xc', 'Yc', 'Ze', 'Az', 'Ne', 'Np', 'Nmu', 'Nh']\n",
    "    '''\n",
    "    if apply_cuts:\n",
    "        drop_mask = ((features[:, 5] < 18)\n",
    "                     * (features[:, 7] > 4.8)\n",
    "                     * (features[:, 8] > 3.6)\n",
    "                     * (features[:, 9] < 1.48)\n",
    "                     * (features[:, 9] > 0.2))\n",
    "        matrices = matrices[drop_mask]\n",
    "        features = features[drop_mask]\n",
    "        true_features = true_features[drop_mask]\n",
    "        part_class = part_class[drop_mask]\n",
    "\n",
    "    matrices = matrices.reshape(matrices.shape[0], -1)\n",
    "    vals = np.unique(true_features[:, [0]], axis=0)\n",
    "    random_mask = np.random.default_rng(seed).random(vals.shape)\n",
    "    is_train = np.in1d(true_features[:, [0]],\n",
    "                       vals[random_mask > (test_share + val_share)])\n",
    "    is_test = np.in1d(true_features[:, [0]],\n",
    "                      vals[random_mask < test_share])\n",
    "    is_val = np.invert(is_train + is_test)\n",
    "    del random_mask, vals\n",
    "    matrices_train = matrices[is_train]\n",
    "    matrices_test = matrices[is_test]\n",
    "    matrices_val = matrices[is_val]\n",
    "    del matrices\n",
    "    if digitize:\n",
    "        digitize_depth = 1000000\n",
    "        arr = np.array(random.choices(matrices_train, k=100000))\n",
    "        splits = np.array_split(np.sort(arr.ravel()), digitize_depth * 2)\n",
    "        cutoffs = [x[-1] for x in splits][:-1]\n",
    "        discrete = np.digitize(matrices_train, cutoffs, right=True)\n",
    "        matrices_train = discrete / digitize_depth - 1\n",
    "        discrete = np.digitize(matrices_test, cutoffs, right=True)\n",
    "        matrices_test = discrete / digitize_depth - 1\n",
    "        discrete = np.digitize(matrices_val, cutoffs, right=True)\n",
    "        matrices_val = discrete / digitize_depth - 1\n",
    "        del arr, splits, discrete, digitize_depth\n",
    "    else:\n",
    "        cutoffs = None\n",
    "    class_train = part_class[is_train]\n",
    "    class_test = part_class[is_test]\n",
    "    class_val = part_class[is_val]\n",
    "    features_train = features[is_train]\n",
    "    features_test = features[is_test]\n",
    "    features_val = features[is_val]\n",
    "    true_features_train = true_features[is_train]\n",
    "    true_features_test = true_features[is_test]\n",
    "    true_features_val = true_features[is_val]\n",
    "    del true_features, part_class\n",
    "    gc.collect()\n",
    "    return (matrices_train, matrices_test, matrices_val,\n",
    "            class_train, class_test, class_val,\n",
    "            features_train, features_test, features_val, cutoffs,\n",
    "            true_features_train, true_features_test, true_features_val)"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "KnwqgsjTlUDQpVlJnH7F9L",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evalute_predictions(test_preds, true_features, title=''):\n",
    "    test_true = true_features[:, 1]\n",
    "    test_energy = true_features[:, 0]\n",
    "    test_theta = true_features[:, 4]\n",
    "\n",
    "    # Plot figures\n",
    "    fig, axs = plt.subplots(2, 3, tight_layout=True, figsize=(21, 12))\n",
    "    axs = axs.flatten()\n",
    "    nbins = 8\n",
    "    bins_range = (14, 18)\n",
    "    E = test_energy\n",
    "    N, energy_bins = np.histogram(E, bins=nbins, range=bins_range)\n",
    "    energy_bins_centers = energy_bins[1:] - 0.5 * (energy_bins[1] - energy_bins[0])\n",
    "    energy_bins_half_width = 0.5 * (energy_bins[1] - energy_bins[0])\n",
    "\n",
    "    for i, (angle_min, angle_max) in zip(range(3), ((0, 20), (20, 40), (40, 60))):\n",
    "        cond = np.where((test_theta >= angle_min) & (test_theta < angle_max) & (test_true == 0) & (test_preds == 0))\n",
    "        correctly_predicted_gamma = \\\n",
    "            binned_statistic(E[cond], test_preds[cond], statistic='count', bins=nbins, range=bins_range)[0]\n",
    "\n",
    "        cond = np.where((test_theta >= angle_min) & (test_theta < angle_max) & (test_preds == 0))\n",
    "        all_predicted_gamma = \\\n",
    "            binned_statistic(E[cond], test_preds[cond], statistic='count', bins=nbins, range=bins_range)[0]\n",
    "\n",
    "        cond = np.where((test_theta >= angle_min) & (test_theta < angle_max) & (test_true == 0))\n",
    "        all_true_gamma = binned_statistic(E[cond], test_preds[cond], statistic='count', bins=nbins, range=bins_range)[0]\n",
    "\n",
    "        survival_fraction_gamma = correctly_predicted_gamma / (all_true_gamma + 0.001)\n",
    "        survival_fraction_gamma_error = np.sqrt(correctly_predicted_gamma) / (all_true_gamma + 0.001)\n",
    "\n",
    "        axs[i].errorbar(energy_bins_centers,\n",
    "                        survival_fraction_gamma,\n",
    "                        xerr=energy_bins_half_width,\n",
    "                        yerr=survival_fraction_gamma_error,\n",
    "                        fmt='o', capsize=3, capthick=3, ms=10, label='$ \\\\gamma $')\n",
    "\n",
    "        protons_predicted_as_gamma = all_predicted_gamma - correctly_predicted_gamma\n",
    "        protons_predicted_as_gamma_uplim = protons_predicted_as_gamma == 0\n",
    "        protons_predicted_as_gamma[protons_predicted_as_gamma_uplim] = 1\n",
    "\n",
    "        cond = np.where((test_theta >= angle_min) & (test_theta < angle_max) & (test_true == 1))\n",
    "        all_true_protons = binned_statistic(E[cond], test_preds[cond], statistic='count', bins=nbins, range=bins_range)[\n",
    "            0]\n",
    "\n",
    "        survival_fraction_protons = protons_predicted_as_gamma / (all_true_protons + 0.001)\n",
    "        survival_fraction_protons_error = np.sqrt(protons_predicted_as_gamma) / (all_true_protons + 0.001)\n",
    "\n",
    "        survival_fraction_protons_error[protons_predicted_as_gamma_uplim] = 0.5 / (all_true_protons[\n",
    "                                                                                       protons_predicted_as_gamma_uplim] + 0.0001)\n",
    "\n",
    "        axs[i].errorbar(energy_bins_centers,\n",
    "                        survival_fraction_protons,\n",
    "                        xerr=energy_bins_half_width,\n",
    "                        yerr=survival_fraction_protons_error,\n",
    "                        uplims=protons_predicted_as_gamma_uplim,\n",
    "                        fmt='o', capsize=3, capthick=3, ms=10, label='$p$')\n",
    "\n",
    "        axs[i].semilogy()\n",
    "        axs[i].legend(fontsize=17)\n",
    "        axs[i].xaxis.set_tick_params(labelsize=17)\n",
    "        axs[i].yaxis.set_tick_params(labelsize=17)\n",
    "        axs[i].set_xlabel('', fontsize=20)\n",
    "        axs[i].set_ylabel('survival fraction', fontsize=20)\n",
    "        axs[i].set_title(f'$ \\\\theta $ range: {angle_min} - {angle_max} deg', fontsize=15)\n",
    "        fig.suptitle('upper: survival fraction of $ \\\\gamma $ and p vs energy\\nlower: energy spectra of the events',\n",
    "                     fontsize=25)\n",
    "\n",
    "    for i, (angle_min, angle_max) in zip(range(3, 6), ((0, 20), (20, 40), (40, 60))):\n",
    "        cond = np.where((test_theta >= angle_min) & (test_theta < angle_max) & (test_true == 0))\n",
    "        sns.histplot(x=E[cond],\n",
    "                     bins=nbins,\n",
    "                     binrange=bins_range,\n",
    "                     log_scale=(False, False),\n",
    "                     element='step',\n",
    "                     fill=False,\n",
    "                     lw=3,\n",
    "                     ax=axs[i],\n",
    "                     label='$ \\\\gamma $')\n",
    "\n",
    "        cond = np.where((test_theta >= angle_min) & (test_theta < angle_max) & (test_true == 1))\n",
    "        sns.histplot(x=E[cond],\n",
    "                     bins=nbins,\n",
    "                     binrange=bins_range,\n",
    "                     log_scale=(False, False),\n",
    "                     element='step',\n",
    "                     fill=False,\n",
    "                     lw=3,\n",
    "                     ax=axs[i],\n",
    "                     label='$ p $')\n",
    "\n",
    "        axs[i].semilogy()\n",
    "        axs[i].legend(fontsize=17)\n",
    "        axs[i].xaxis.set_tick_params(labelsize=17)\n",
    "        axs[i].yaxis.set_tick_params(labelsize=17)\n",
    "        axs[i].set_xlabel('lg($E_0$/eV)', fontsize=20)\n",
    "        axs[i].set_ylabel('number of events', fontsize=20)"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "j6FCialIDttelf0PomH7Si",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "(matrices_train, matrices_test, matrices_val,\n",
    " class_train, class_test, class_val,\n",
    " features_train, features_test, features_val, cutoffs,\n",
    " true_features_train, true_features_test, true_features_val) = load_data(mode='combined_gm_pr',\n",
    "                                                                         test_share=0.2,\n",
    "                                                                         val_share=0.2)\n",
    "matrices_train = np.concatenate([matrices_train, features_train[:, [5, 6]]], axis=1)\n",
    "matrices_test = np.concatenate([matrices_test, features_test[:, [5, 6]]], axis=1)\n",
    "matrices_val = np.concatenate([matrices_val, features_val[:, [5, 6]]], axis=1)\n",
    "\n",
    "mean = matrices_train.mean(axis=0)\n",
    "std = matrices_train.std(axis=0)\n",
    "std[std == 0] = 1\n",
    "matrices_train = (matrices_train - mean) / std\n",
    "matrices_test = (matrices_test - mean) / std\n",
    "matrices_val = (matrices_val - mean) / std"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "SDmYJQGyBzwwXw1A8IeKMe",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "        self.dense1 = nn.Linear(514, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.activation1 = nn.SELU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.dense2 = nn.Linear(512, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.activation2 = nn.SELU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.out_layer = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.activation1(self.bn1(self.dense1(x))))\n",
    "        x = self.dropout2(self.activation2(self.bn2(self.dense2(x))))\n",
    "        return torch.softmax(self.out_layer(x), dim=1)"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "eHPAzSVj5pf3oJtbLMwyOu",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = CustomNetwork().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "tensorboard_writer = SummaryWriter(log_dir='logs/baseline_cnn')\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(matrices_train, dtype=torch.float32),\n",
    "                         torch.tensor(class_train, dtype=torch.long))\n",
    "train_loader = DataLoader(train_ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "val_ds = TensorDataset(torch.tensor(matrices_val, dtype=torch.float32),\n",
    "                       torch.tensor(class_val, dtype=torch.long))\n",
    "val_loader = DataLoader(val_ds, batch_size=1024, shuffle=False)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold=0.0001)"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "wuYMyGAVylcYNQOtSLhFZs",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "best_val_accuracy = 0\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    train_loss, train_corrects, train_accuracy = 0, 0, 0\n",
    "    val_loss, val_corrects, val_accuracy = 0, 0, 0\n",
    "\n",
    "    # Calculate the accuracy over training\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            train_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            train_corrects += pred.eq(target.view_as(pred)).sum().item()\n",
    "    train_accuracy = 100. * train_corrects / len(train_loader.dataset)\n",
    "\n",
    "    # Calculate the accuracy over validation\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            val_corrects += pred.eq(target.view_as(pred)).sum().item()\n",
    "    val_accuracy = 100. * val_corrects / len(val_loader.dataset)\n",
    "\n",
    "    print(\"Epoch: {}, Training Loss: {:.5f}, Training Accuracy: {:.2f}%, Validation Loss: {:.5f}, Validation Accuracy: {:.2f}%\".format(\n",
    "        epoch, train_loss / len(train_loader), train_accuracy, val_loss / len(val_loader), val_accuracy))\n",
    "\n",
    "    tensorboard_writer.add_scalar('train_loss', train_loss / len(train_loader), epoch)\n",
    "    tensorboard_writer.add_scalar('train_accuracy', train_accuracy, epoch)\n",
    "    tensorboard_writer.add_scalar('val_loss', val_loss / len(val_loader), epoch)\n",
    "    tensorboard_writer.add_scalar('val_accuracy', val_accuracy, epoch)\n",
    "\n",
    "    # Implementing EarlyStopping and ModelCheckpoint\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_weights.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= 22:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    scheduler.step(val_accuracy)\n",
    "\n",
    "tensorboard_writer.close()\n"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "kA21QjAT7jBhMyig3HkMov",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.load_state_dict(torch.load('best_weights.pth'))\n",
    "test_ds = TensorDataset(torch.tensor(matrices_test, dtype=torch.float32),\n",
    "                        torch.tensor(class_test, dtype=torch.long))\n",
    "test_loader = DataLoader(test_ds, batch_size=1024, shuffle=False)\n",
    "test_corrects, test_accuracy = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1)\n",
    "        test_corrects += pred.eq(target.view_as(pred)).sum().item()\n",
    "test_accuracy = 100. * test_corrects / len(test_loader.dataset)\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_accuracy))\n",
    "\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for data, _ in test_loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        preds += torch.argmax(output, dim=1).cpu().numpy().tolist()\n",
    "preds = np.array(preds)\n",
    "evalute_predictions(preds, true_features_test)"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "aFGv78IebwwViCnxn5z7Nx",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python"
  },
  "datalore": {
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "base_environment": "default",
   "packages": [],
   "report_row_ids": [],
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
